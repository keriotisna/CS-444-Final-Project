{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from icecream import ic\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm # Progress bar for training\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from dataLoading import CIFAR10Dataset, displayImageGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def validateModelIO(model:nn.Module):\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(model)\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters.\")\n",
    "    print(summary(model, input_size=(3, 32, 32)))\n",
    "\n",
    "    dummy_input = torch.randn(1, 3, 32, 32, device=device, dtype=torch.float)\n",
    "    output = model(dummy_input)\n",
    "    assert output.size() == (1, 10), f\"Expected output size (1, 10), got {output.size()}!\"\n",
    "    print(\"Test passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN for classifying images in the AnimalDataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(32 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # for layer in self.fc:\n",
    "        #     x = layer(x)\n",
    "        #     print(type(x).__class__.__name__)\n",
    "        #     print(x.size())\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    printOutsize = False\n",
    "    \n",
    "    def __init__(self, channelCount, activation:nn.Module=nn.ReLU(), kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channelCount, out_channels=channelCount, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channelCount, out_channels=channelCount, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.printOutsize:\n",
    "            print(x.size())\n",
    "            \n",
    "        y1 = self.c1(x)\n",
    "        \n",
    "        if self.printOutsize:\n",
    "            print(y1.size())\n",
    "            \n",
    "        y = self.c2(y1)\n",
    "        \n",
    "        if self.printOutsize:\n",
    "            print(str(y.size()) + '\\n')\n",
    "            \n",
    "        y = y + x\n",
    "        \n",
    "        self.outsize = y.size()\n",
    "        \n",
    "        return self.activation(y)\n",
    "\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, fc:nn.Sequential, printOutsize=False):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        ResidualBlock.printOutsize = printOutsize\n",
    "        \n",
    "        self.fc = fc\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Apply the layers defined in `__init__` in order.\n",
    "        Args:\n",
    "            x (Tensor): The input tensor of shape (N, 3, 256, 256).\n",
    "        Returns:\n",
    "            output (Tensor): The output tensor of shape (N, 10).\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=144, out_features=10),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "\n",
    "fc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=144, out_features=10)\n",
    "        )\n",
    "\n",
    "# TODO: Change this\n",
    "fc2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(channelCount=16),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=144, out_features=10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ResidualCNN(\n",
      "  (fc): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (14): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU()\n",
      "    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (20): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): ReLU()\n",
      "    (22): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU()\n",
      "    (25): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (26): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU()\n",
      "    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU()\n",
      "    (31): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (32): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (33): ReLU()\n",
      "    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU()\n",
      "    (37): ResidualBlock(\n",
      "      (c1): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (c2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (38): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU()\n",
      "    (40): Flatten(start_dim=1, end_dim=-1)\n",
      "    (41): Linear(in_features=144, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 48714 parameters.\n",
      "torch.Size([2, 16, 30, 30])\n",
      "torch.Size([2, 16, 30, 30])\n",
      "torch.Size([2, 16, 30, 30])\n",
      "\n",
      "torch.Size([2, 16, 13, 13])\n",
      "torch.Size([2, 16, 13, 13])\n",
      "torch.Size([2, 16, 13, 13])\n",
      "\n",
      "torch.Size([2, 16, 11, 11])\n",
      "torch.Size([2, 16, 11, 11])\n",
      "torch.Size([2, 16, 11, 11])\n",
      "\n",
      "torch.Size([2, 16, 9, 9])\n",
      "torch.Size([2, 16, 9, 9])\n",
      "torch.Size([2, 16, 9, 9])\n",
      "\n",
      "torch.Size([2, 16, 7, 7])\n",
      "torch.Size([2, 16, 7, 7])\n",
      "torch.Size([2, 16, 7, 7])\n",
      "\n",
      "torch.Size([2, 16, 5, 5])\n",
      "torch.Size([2, 16, 5, 5])\n",
      "torch.Size([2, 16, 5, 5])\n",
      "\n",
      "torch.Size([2, 16, 3, 3])\n",
      "torch.Size([2, 16, 3, 3])\n",
      "torch.Size([2, 16, 3, 3])\n",
      "\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 30, 30]             448\n",
      "            Conv2d-2           [-1, 16, 30, 30]           2,320\n",
      "              ReLU-3           [-1, 16, 30, 30]               0\n",
      "            Conv2d-4           [-1, 16, 30, 30]           2,320\n",
      "              ReLU-5           [-1, 16, 30, 30]               0\n",
      "              ReLU-6           [-1, 16, 30, 30]               0\n",
      "              ReLU-7           [-1, 16, 30, 30]               0\n",
      "              ReLU-8           [-1, 16, 30, 30]               0\n",
      "              ReLU-9           [-1, 16, 30, 30]               0\n",
      "             ReLU-10           [-1, 16, 30, 30]               0\n",
      "             ReLU-11           [-1, 16, 30, 30]               0\n",
      "             ReLU-12           [-1, 16, 30, 30]               0\n",
      "    ResidualBlock-13           [-1, 16, 30, 30]               0\n",
      "      BatchNorm2d-14           [-1, 16, 30, 30]              32\n",
      "        MaxPool2d-15           [-1, 16, 15, 15]               0\n",
      "           Conv2d-16           [-1, 16, 13, 13]           2,320\n",
      "      BatchNorm2d-17           [-1, 16, 13, 13]              32\n",
      "             ReLU-18           [-1, 16, 13, 13]               0\n",
      "           Conv2d-19           [-1, 16, 13, 13]           2,320\n",
      "             ReLU-20           [-1, 16, 13, 13]               0\n",
      "           Conv2d-21           [-1, 16, 13, 13]           2,320\n",
      "             ReLU-22           [-1, 16, 13, 13]               0\n",
      "             ReLU-23           [-1, 16, 13, 13]               0\n",
      "             ReLU-24           [-1, 16, 13, 13]               0\n",
      "             ReLU-25           [-1, 16, 13, 13]               0\n",
      "             ReLU-26           [-1, 16, 13, 13]               0\n",
      "             ReLU-27           [-1, 16, 13, 13]               0\n",
      "             ReLU-28           [-1, 16, 13, 13]               0\n",
      "             ReLU-29           [-1, 16, 13, 13]               0\n",
      "    ResidualBlock-30           [-1, 16, 13, 13]               0\n",
      "      BatchNorm2d-31           [-1, 16, 13, 13]              32\n",
      "             ReLU-32           [-1, 16, 13, 13]               0\n",
      "           Conv2d-33           [-1, 16, 11, 11]           2,320\n",
      "      BatchNorm2d-34           [-1, 16, 11, 11]              32\n",
      "             ReLU-35           [-1, 16, 11, 11]               0\n",
      "           Conv2d-36           [-1, 16, 11, 11]           2,320\n",
      "             ReLU-37           [-1, 16, 11, 11]               0\n",
      "           Conv2d-38           [-1, 16, 11, 11]           2,320\n",
      "             ReLU-39           [-1, 16, 11, 11]               0\n",
      "             ReLU-40           [-1, 16, 11, 11]               0\n",
      "             ReLU-41           [-1, 16, 11, 11]               0\n",
      "             ReLU-42           [-1, 16, 11, 11]               0\n",
      "             ReLU-43           [-1, 16, 11, 11]               0\n",
      "             ReLU-44           [-1, 16, 11, 11]               0\n",
      "             ReLU-45           [-1, 16, 11, 11]               0\n",
      "             ReLU-46           [-1, 16, 11, 11]               0\n",
      "    ResidualBlock-47           [-1, 16, 11, 11]               0\n",
      "      BatchNorm2d-48           [-1, 16, 11, 11]              32\n",
      "             ReLU-49           [-1, 16, 11, 11]               0\n",
      "           Conv2d-50             [-1, 16, 9, 9]           2,320\n",
      "      BatchNorm2d-51             [-1, 16, 9, 9]              32\n",
      "             ReLU-52             [-1, 16, 9, 9]               0\n",
      "           Conv2d-53             [-1, 16, 9, 9]           2,320\n",
      "             ReLU-54             [-1, 16, 9, 9]               0\n",
      "           Conv2d-55             [-1, 16, 9, 9]           2,320\n",
      "             ReLU-56             [-1, 16, 9, 9]               0\n",
      "             ReLU-57             [-1, 16, 9, 9]               0\n",
      "             ReLU-58             [-1, 16, 9, 9]               0\n",
      "             ReLU-59             [-1, 16, 9, 9]               0\n",
      "             ReLU-60             [-1, 16, 9, 9]               0\n",
      "             ReLU-61             [-1, 16, 9, 9]               0\n",
      "             ReLU-62             [-1, 16, 9, 9]               0\n",
      "             ReLU-63             [-1, 16, 9, 9]               0\n",
      "    ResidualBlock-64             [-1, 16, 9, 9]               0\n",
      "      BatchNorm2d-65             [-1, 16, 9, 9]              32\n",
      "             ReLU-66             [-1, 16, 9, 9]               0\n",
      "           Conv2d-67             [-1, 16, 7, 7]           2,320\n",
      "      BatchNorm2d-68             [-1, 16, 7, 7]              32\n",
      "             ReLU-69             [-1, 16, 7, 7]               0\n",
      "           Conv2d-70             [-1, 16, 7, 7]           2,320\n",
      "             ReLU-71             [-1, 16, 7, 7]               0\n",
      "           Conv2d-72             [-1, 16, 7, 7]           2,320\n",
      "             ReLU-73             [-1, 16, 7, 7]               0\n",
      "             ReLU-74             [-1, 16, 7, 7]               0\n",
      "             ReLU-75             [-1, 16, 7, 7]               0\n",
      "             ReLU-76             [-1, 16, 7, 7]               0\n",
      "             ReLU-77             [-1, 16, 7, 7]               0\n",
      "             ReLU-78             [-1, 16, 7, 7]               0\n",
      "             ReLU-79             [-1, 16, 7, 7]               0\n",
      "             ReLU-80             [-1, 16, 7, 7]               0\n",
      "    ResidualBlock-81             [-1, 16, 7, 7]               0\n",
      "      BatchNorm2d-82             [-1, 16, 7, 7]              32\n",
      "             ReLU-83             [-1, 16, 7, 7]               0\n",
      "           Conv2d-84             [-1, 16, 5, 5]           2,320\n",
      "      BatchNorm2d-85             [-1, 16, 5, 5]              32\n",
      "             ReLU-86             [-1, 16, 5, 5]               0\n",
      "           Conv2d-87             [-1, 16, 5, 5]           2,320\n",
      "             ReLU-88             [-1, 16, 5, 5]               0\n",
      "           Conv2d-89             [-1, 16, 5, 5]           2,320\n",
      "             ReLU-90             [-1, 16, 5, 5]               0\n",
      "             ReLU-91             [-1, 16, 5, 5]               0\n",
      "             ReLU-92             [-1, 16, 5, 5]               0\n",
      "             ReLU-93             [-1, 16, 5, 5]               0\n",
      "             ReLU-94             [-1, 16, 5, 5]               0\n",
      "             ReLU-95             [-1, 16, 5, 5]               0\n",
      "             ReLU-96             [-1, 16, 5, 5]               0\n",
      "             ReLU-97             [-1, 16, 5, 5]               0\n",
      "    ResidualBlock-98             [-1, 16, 5, 5]               0\n",
      "      BatchNorm2d-99             [-1, 16, 5, 5]              32\n",
      "            ReLU-100             [-1, 16, 5, 5]               0\n",
      "          Conv2d-101             [-1, 16, 3, 3]           2,320\n",
      "     BatchNorm2d-102             [-1, 16, 3, 3]              32\n",
      "            ReLU-103             [-1, 16, 3, 3]               0\n",
      "          Conv2d-104             [-1, 16, 3, 3]           2,320\n",
      "            ReLU-105             [-1, 16, 3, 3]               0\n",
      "          Conv2d-106             [-1, 16, 3, 3]           2,320\n",
      "            ReLU-107             [-1, 16, 3, 3]               0\n",
      "            ReLU-108             [-1, 16, 3, 3]               0\n",
      "            ReLU-109             [-1, 16, 3, 3]               0\n",
      "            ReLU-110             [-1, 16, 3, 3]               0\n",
      "            ReLU-111             [-1, 16, 3, 3]               0\n",
      "            ReLU-112             [-1, 16, 3, 3]               0\n",
      "            ReLU-113             [-1, 16, 3, 3]               0\n",
      "            ReLU-114             [-1, 16, 3, 3]               0\n",
      "   ResidualBlock-115             [-1, 16, 3, 3]               0\n",
      "     BatchNorm2d-116             [-1, 16, 3, 3]              32\n",
      "            ReLU-117             [-1, 16, 3, 3]               0\n",
      "         Flatten-118                  [-1, 144]               0\n",
      "          Linear-119                   [-1, 10]           1,450\n",
      "================================================================\n",
      "Total params: 48,714\n",
      "Trainable params: 48,714\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.51\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 2.71\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "torch.Size([1, 16, 30, 30])\n",
      "torch.Size([1, 16, 30, 30])\n",
      "torch.Size([1, 16, 30, 30])\n",
      "\n",
      "torch.Size([1, 16, 13, 13])\n",
      "torch.Size([1, 16, 13, 13])\n",
      "torch.Size([1, 16, 13, 13])\n",
      "\n",
      "torch.Size([1, 16, 11, 11])\n",
      "torch.Size([1, 16, 11, 11])\n",
      "torch.Size([1, 16, 11, 11])\n",
      "\n",
      "torch.Size([1, 16, 9, 9])\n",
      "torch.Size([1, 16, 9, 9])\n",
      "torch.Size([1, 16, 9, 9])\n",
      "\n",
      "torch.Size([1, 16, 7, 7])\n",
      "torch.Size([1, 16, 7, 7])\n",
      "torch.Size([1, 16, 7, 7])\n",
      "\n",
      "torch.Size([1, 16, 5, 5])\n",
      "torch.Size([1, 16, 5, 5])\n",
      "torch.Size([1, 16, 5, 5])\n",
      "\n",
      "torch.Size([1, 16, 3, 3])\n",
      "torch.Size([1, 16, 3, 3])\n",
      "torch.Size([1, 16, 3, 3])\n",
      "\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validateModelIO(ResidualCNN(fc=fc1, printOutsize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4919, 0.4827, 0.4468])\n",
      "tensor([0.2024, 0.1995, 0.2011])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create dataset instances\n",
    "fullDataset = CIFAR10Dataset(rootDirectory='cifar-10', csvFilename='trainLabels.csv', dataFolder='train', transform=None)\n",
    "TRAIN_DATASET, VALIDATION_DATSET, TEST_DATSAET = random_split(fullDataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "trainLoader = DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in trainLoader:\n",
    "    \n",
    "    batchSamples = images.size(0)\n",
    "    # Get an image view of shape (batchSamples, C, H*W) which is faster than a transpose as we don't shift any data\n",
    "    images = images.view(batchSamples, images.size(1), -1)\n",
    "    # Calculate total mean and total std over dim=2\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "# Divide means and stdevs by number of samples\n",
    "mean /= len(trainLoader.dataset)\n",
    "std /= len(trainLoader.dataset)\n",
    "print(mean)\n",
    "print(std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoAugmentCIFAR = tv.transforms.Compose([\n",
    "    v2.AutoAugment(v2.AutoAugmentPolicy.CIFAR10),\n",
    "])\n",
    "\n",
    "# Update transform with normalized values\n",
    "transform = tv.transforms.Compose([\n",
    "    CIFAR10Dataset.defaultTransform,\n",
    "    # Add other transforms here\n",
    "    v2.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "fullDataset = CIFAR10Dataset(rootDirectory='cifar-10', csvFilename='trainLabels.csv', dataFolder='train', transform=transform)\n",
    "TRAIN_DATASET, VALIDATION_DATSET, TEST_DATSAET = random_split(fullDataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "trainLoader = DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainLoss: 0.5924, trainAccuracy: 0.7930: 100%|██████████| 60/60 [25:38<00:00, 25.65s/it]\n"
     ]
    }
   ],
   "source": [
    "def trainEpoch(model:nn.Module, dataloader:DataLoader, optimizer:torch.optim):\n",
    "    \n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    \n",
    "    totalLoss = 0\n",
    "    \n",
    "    N = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for features, labels in dataloader:\n",
    "        x, y = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        forwardPass = model.forward(x)\n",
    "        \n",
    "        # This adds the current accuracy to correct which is averaged over all iterations of the epoch.\n",
    "        correct += (forwardPass.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "        loss = lossFunction(forwardPass, y)\n",
    "        totalLoss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        N += 1\n",
    "    \n",
    "    return totalLoss / N, correct / N\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 60\n",
    "RUN = '1-ResNetfc1'\n",
    "RUNS_DIR = 'runs'\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.9\n",
    "model = ResidualCNN(fc=fc1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(f'{RUNS_DIR}/run{RUN}_batch{BATCH_SIZE}_lr{lr:0.6f}', flush_secs=10)\n",
    "\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "for epoch in pbar:\n",
    "    trainLoss, trainAccuracy = trainEpoch(model=model, dataloader=trainLoader, optimizer=optimizer)\n",
    "\n",
    "    pbar.set_description(\"trainLoss: {:.4f}, trainAccuracy: {:.4f}\".format(trainLoss, trainAccuracy), refresh=True)\n",
    "    writer.add_scalar('trainLoss', trainLoss, epoch)\n",
    "    writer.add_scalar('trainAccuracy', trainAccuracy, epoch)\n",
    "    writer.add_scalar('lr', scheduler.get_last_lr(), epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "# pbar = tqdm(range(EPOCHS))\n",
    "# for epoch in pbar:\n",
    "#     train_loss, train_acc = trainEpoch(model=model, dataloader=trainLoader, optimizer=optimizer)\n",
    "#     # val_loss, val_acc = self.val_epoch()\n",
    "#     # self.writer.add_scalar('lr', self.lr_schedule.get_last_lr(), epoch)\n",
    "#     # self.writer.add_scalar('val_acc', val_acc, epoch)\n",
    "#     # self.writer.add_scalar('val_loss', val_loss, epoch)\n",
    "#     # self.writer.add_scalar('train_acc', train_acc, epoch)\n",
    "#     writer.add_scalar('train_loss', train_loss, epoch)\n",
    "#     # pbar.set_description(\"val acc: {:.4f}, train acc: {:.4f}\".format(val_acc, train_acc), refresh=True)\n",
    "#     # if val_acc > best_val_acc:\n",
    "#     #     best_val_acc = val_acc\n",
    "#     #     best_epoch = epoch\n",
    "#     #     torch.save(self.model.state_dict(), model_file_name)\n",
    "#     # self.lr_schedule.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desktopConda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
